#!/usr/bin/env python3


import os
import json
import subprocess
import time
import random
import signal
import sys
import argparse
from datetime import datetime, timedelta
import shutil
import glob
import hashlib
from collections import defaultdict
import re

class ConfigFuzzWafRunnerV2:
    def __init__(self, timeout_seconds=3600, use_cache=True, stack_threshold=512, resume_from=None):
        self.start_time = time.time()
        self.timeout = timeout_seconds
        self.use_cache = use_cache
        self.stack_threshold = stack_threshold
        self.test_count = 0
        self.success_count = 0
        self.fail_count = 0
        self.cache_hits = 0
        self.running = True
        self.resumed_session = False
        self.original_start_time = self.start_time
        
        # Ïä§ÌÉù Î≥ÄÌôî Ï∂îÏ†Å
        self.baseline_stack_map = {}  # Í∏∞Ï§Ä Ïä§ÌÉù ÏÇ¨Ïö©Îüâ
        self.stack_changes_count = 0
        self.significant_changes = []
        
        # Í≤ΩÎ°ú ÏÑ§Ï†ï
        self.configfuzz_path = "/conffuzz"
        self.ardupilot_path = "/home/ubuntu/lab/ardupilot"
        self.build_dir = os.path.join(self.ardupilot_path, "build/AIRLink")
        
        # ÏûëÏóÖ ÎîîÎ†âÌÜ†Î¶¨ ÏÑ§Ï†ï
        os.chdir(self.configfuzz_path)
        
        # Resume Ï≤òÎ¶¨
        if resume_from:
            self.resume_session(resume_from)
        else:
            self.init_new_session()
        
        # Îß§ÌÅ¨Î°ú Ï∫êÏãú
        self.macro_cache = {}
        if self.use_cache:
            self.load_cache()
        
        # Îß§ÌÅ¨Î°ú Î°úÎìú
        self.load_macros()
        
        # Signal handler
        signal.signal(signal.SIGINT, self.signal_handler)
        signal.signal(signal.SIGTERM, self.signal_handler)
        
        # ÏãúÏûë Î©îÏãúÏßÄ
        self.print_header()
        
    def resume_session(self, resume_dir):
        """Ïù¥Ï†Ñ ÏÑ∏ÏÖò Ïû¨Í∞ú"""
        # Ï†ÑÏ≤¥ Í≤ΩÎ°ú Íµ¨ÏÑ±
        if not os.path.isabs(resume_dir):
            if os.path.exists(os.path.join(self.configfuzz_path, resume_dir)):
                resume_path = os.path.join(self.configfuzz_path, resume_dir)
            elif os.path.exists(resume_dir):
                resume_path = resume_dir
            else:
                print(f"ERROR: Resume directory not found: {resume_dir}")
                sys.exit(1)
        else:
            resume_path = resume_dir
            
        if not os.path.exists(resume_path):
            print(f"ERROR: Resume directory does not exist: {resume_path}")
            sys.exit(1)
            
        print(f"Resuming from: {resume_path}")
        self.resumed_session = True
        
        # Ïù¥Ï†Ñ ÏÑ∏ÏÖò ÏÉÅÌÉú Î°úÎìú
        state_file = os.path.join(resume_path, "session_state.json")
        if os.path.exists(state_file):
            with open(state_file, 'r') as f:
                state = json.load(f)
                self.test_count = state.get("test_count", 0)
                self.success_count = state.get("success_count", 0)
                self.fail_count = state.get("fail_count", 0)
                self.cache_hits = state.get("cache_hits", 0)
                self.original_start_time = state.get("original_start_time", self.start_time)
                self.baseline_stack_map = state.get("baseline_stack_map", {})
                self.stack_changes_count = state.get("stack_changes_count", 0)
                self.significant_changes = state.get("significant_changes", [])
                elapsed_time = state.get("elapsed_time", 0)
                
                # ÎÇ®ÏùÄ ÏãúÍ∞Ñ Ï°∞Ï†ï (ÏÇ¨Ïö©ÏûêÍ∞Ä Î™ÖÏãúÏ†ÅÏúºÎ°ú ÏãúÍ∞ÑÏùÑ ÏßÄÏ†ïÌïòÏßÄ ÏïäÏúºÎ©¥ Í∏∞Î≥∏Í∞í ÏÇ¨Ïö©)
                if self.timeout == 3600:  # Í∏∞Î≥∏Í∞íÏù¥Î©¥
                    self.timeout = max(3600 - elapsed_time, 60)  # ÏµúÏÜå 60Ï¥à
                else:
                    # ÏÇ¨Ïö©ÏûêÍ∞Ä Î™ÖÏãúÏ†ÅÏúºÎ°ú ÏßÄÏ†ïÌïú Í≤ΩÏö∞ Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©
                    pass
                
                print(f"Loaded state: {self.test_count} tests completed, {self.success_count} successful")
                print(f"Remaining time: {self.format_duration(self.timeout)}")
        else:
            print("Session state file not found, calculating from results...")
            self.calculate_stats_from_results(resume_path)
        
        # Ï∂úÎ†• ÎîîÎ†âÌÜ†Î¶¨Îäî Í∏∞Ï°¥ Í≤ÉÏùÑ ÏÇ¨Ïö©
        self.output_dir_name = os.path.basename(resume_path)
        self.output_dir = resume_path
        
        # ÎîîÎ†âÌÜ†Î¶¨Í∞Ä ÏóÜÏúºÎ©¥ ÏÉùÏÑ±
        os.makedirs(os.path.join(self.output_dir, "configs"), exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, "logs"), exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, "results"), exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, "stack_analysis"), exist_ok=True)
        
        # Î°úÍ∑∏ ÌååÏùº append Î™®ÎìúÎ°ú Ïó¥Í∏∞
        self.main_log_path = f"{self.output_dir}/main.log"
        self.main_log = open(self.main_log_path, "a")
        self.log("\n" + "="*60)
        self.log(f"SESSION RESUMED at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        self.log("="*60)
        
    def calculate_stats_from_results(self, resume_path):
        """Í≤∞Í≥º ÌååÏùºÎì§ÏóêÏÑú ÌÜµÍ≥Ñ Í≥ÑÏÇ∞"""
        # ÌÖåÏä§Ìä∏ Ïàò Í≥ÑÏÇ∞
        config_files = glob.glob(os.path.join(resume_path, "configs", "test_*.json"))
        self.test_count = len(config_files)
        
        # ÏÑ±Í≥µ/Ïã§Ìå® Ïàò Í≥ÑÏÇ∞
        for i in range(self.test_count):
            meta_file = os.path.join(resume_path, f"result_ardupilot_{i}.json_meta_results.json")
            if os.path.exists(meta_file):
                try:
                    with open(meta_file, 'r') as f:
                        meta = json.load(f)
                        if meta.get("build_result"):
                            self.success_count += 1
                        else:
                            self.fail_count += 1
                except:
                    pass
                    
        # main.logÏóêÏÑú Í≤ΩÍ≥º ÏãúÍ∞Ñ Ï∂îÏ†ï
        log_file = os.path.join(resume_path, "main.log")
        if os.path.exists(log_file):
            try:
                with open(log_file, 'r') as f:
                    lines = f.readlines()
                    first_time = None
                    last_time = None
                    for line in lines:
                        match = re.match(r'\[(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})\]', line)
                        if match:
                            timestamp = datetime.strptime(match.group(1), "%Y-%m-%d %H:%M:%S")
                            if first_time is None:
                                first_time = timestamp
                            last_time = timestamp
                    
                    if first_time and last_time:
                        elapsed = (last_time - first_time).total_seconds()
                        # ÏÇ¨Ïö©ÏûêÍ∞Ä Î™ÖÏãúÏ†ÅÏúºÎ°ú ÏãúÍ∞ÑÏùÑ ÏßÄÏ†ïÌïòÏßÄ ÏïäÏúºÎ©¥ Í∏∞Î≥∏Í∞í ÏÇ¨Ïö©
                        if self.timeout == 3600:  # Í∏∞Î≥∏Í∞íÏù¥Î©¥
                            self.timeout = max(3600 - elapsed, 60)  # ÏµúÏÜå 60Ï¥à
                        self.original_start_time = self.start_time - elapsed
            except:
                pass
                
    def init_new_session(self):
        """ÏÉà ÏÑ∏ÏÖò Ï¥àÍ∏∞Ìôî"""
        # Ï∂úÎ†• ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±
        self.output_dir_name = f"output_waf_v2_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.output_dir = os.path.join(self.configfuzz_path, self.output_dir_name)
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, "configs"), exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, "logs"), exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, "results"), exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, "stack_analysis"), exist_ok=True)
        
        # Î°úÍ∑∏ ÌååÏùº
        self.main_log_path = f"{self.output_dir}/main.log"
        self.main_log = open(self.main_log_path, "w")
        
    def save_session_state(self):
        """ÌòÑÏû¨ ÏÑ∏ÏÖò ÏÉÅÌÉú Ï†ÄÏû•"""
        state = {
            "test_count": self.test_count,
            "success_count": self.success_count,
            "fail_count": self.fail_count,
            "cache_hits": self.cache_hits,
            "original_start_time": self.original_start_time,
            "elapsed_time": time.time() - self.original_start_time,
            "baseline_stack_map": self.baseline_stack_map,
            "stack_changes_count": self.stack_changes_count,
            "significant_changes": self.significant_changes,
            "last_update": datetime.now().isoformat()
        }
        
        state_file = os.path.join(self.output_dir, "session_state.json")
        with open(state_file, 'w') as f:
            json.dump(state, f, indent=2)
            
    def print_header(self):
        """Ìó§Îçî Ï†ïÎ≥¥ Ï∂úÎ†•"""
        duration_str = self.format_duration(self.timeout)
        
        self.log("=" * 60)
        if self.resumed_session:
            self.log("ConfigFuzz + Waf Integration v2 [RESUMED SESSION]")
        else:
            self.log("ConfigFuzz + Waf Integration v2")
        self.log("=" * 60)
        self.log(f"Output directory: {self.output_dir_name}")
        self.log(f"Duration: {duration_str}")
        self.log(f"Stack threshold: {self.stack_threshold} bytes")
        self.log(f"Cache: {'Enabled' if self.use_cache else 'Disabled'}")
        if self.resumed_session:
            self.log(f"Tests completed: {self.test_count}")
            self.log(f"Success rate so far: {self.success_count/self.test_count*100:.1f}%" if self.test_count > 0 else "N/A")
        self.log(f"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        self.log(f"Will end at: {(datetime.now() + timedelta(seconds=self.timeout)).strftime('%Y-%m-%d %H:%M:%S')}")
        self.log("=" * 60)
        
    def format_duration(self, seconds):
        """ÏãúÍ∞ÑÏùÑ ÏùΩÍ∏∞ Ïâ¨Ïö¥ ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò"""
        hours = seconds // 3600
        minutes = (seconds % 3600) // 60
        secs = seconds % 60
        
        parts = []
        if hours > 0:
            parts.append(f"{int(hours)} hour{'s' if hours != 1 else ''}")
        if minutes > 0:
            parts.append(f"{int(minutes)} minute{'s' if minutes != 1 else ''}")
        if secs > 0 or len(parts) == 0:
            parts.append(f"{int(secs)} second{'s' if secs != 1 else ''}")
        
        return " ".join(parts)
        
    def signal_handler(self, sig, frame):
        """Ctrl+C Ï≤òÎ¶¨"""
        self.log("\nInterrupted by user. Saving results...")
        self.running = False
        self.save_session_state()
        self.save_final_report()
        sys.exit(0)
        
    def log(self, message):
        """Î°úÍ∑∏ Ï∂úÎ†• Î∞è Ï†ÄÏû•"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_msg = f"[{timestamp}] {message}"
        print(log_msg)
        self.main_log.write(log_msg + "\n")
        self.main_log.flush()
        
    def load_cache(self):
        """Ï∫êÏãú Î°úÎìú (ÌòÑÏû¨ ÏÑ∏ÏÖò + Îã§Î•∏ ÏÑ∏ÏÖòÎì§)"""
        cache_loaded = 0
        
        # 1. ÌòÑÏû¨ ÏÑ∏ÏÖòÏùò Ï∫êÏãú Î°úÎìú
        current_cache_file = os.path.join(self.output_dir, "macro_cache.json")
        if os.path.exists(current_cache_file):
            try:
                with open(current_cache_file, 'r') as f:
                    self.macro_cache = json.load(f)
                    cache_loaded = len(self.macro_cache)
                    self.log(f"Loaded {cache_loaded} cached results from current session")
            except Exception as e:
                self.log(f"Error loading current cache: {e}")
        
        # 2. Îã§Î•∏ ÏÑ∏ÏÖòÎì§Ïùò Ï∫êÏãúÎèÑ Î°úÎìú
        try:
            for dir_name in os.listdir(self.configfuzz_path):
                if dir_name.startswith("output_") and dir_name != self.output_dir_name:
                    cache_file = os.path.join(self.configfuzz_path, dir_name, "macro_cache.json")
                    if os.path.exists(cache_file):
                        with open(cache_file, 'r') as f:
                            cached = json.load(f)
                            # Í∏∞Ï°¥ Ï∫êÏãúÏóê ÏóÜÎäî Ìï≠Î™©Îßå Ï∂îÍ∞Ä
                            for key, value in cached.items():
                                if key not in self.macro_cache:
                                    self.macro_cache[key] = value
                                    cache_loaded += 1
            
            if cache_loaded > 0:
                self.log(f"Total {len(self.macro_cache)} cached results loaded")
        except Exception as e:
            self.log(f"Cache loading error: {e}")
            
    def load_macros(self):
        """ConfigFuzz Îß§ÌÅ¨Î°ú Î°úÎìú"""
        macro_file = os.path.join(self.configfuzz_path, "src/adapter/ardupilot/macros.json")
        try:
            with open(macro_file, 'r') as f:
                self.all_macros = json.load(f)
        except Exception as e:
            self.log(f"Error loading macros: {e}")
            self.all_macros = {}
        
        # ÏïàÏ†ÑÌïú Îß§ÌÅ¨Î°úÎßå ÏÑ†ÌÉù (Î≤ÑÌçº ÌÅ¨Í∏∞ Í¥ÄÎ†® Îß§ÌÅ¨Î°ú Ïö∞ÏÑ†)
        self.safe_macros = []
        priority_keywords = ['BUFLEN', 'SIZE', 'MAX', 'STACK', 'ENABLED', 'DISABLE', 'HAL_', 'AP_', 'CONFIG']
        
        for name, info in self.all_macros.items():
            if name != "PACKED" and isinstance(info, dict):
                # Ïö∞ÏÑ†ÏàúÏúÑ Îß§ÌÅ¨Î°ú
                if any(keyword in name for keyword in priority_keywords):
                    self.safe_macros.append(name)
        
        # ÏµúÎåÄ 200Í∞úÎ°ú Ï†úÌïú
        self.safe_macros = self.safe_macros[:200]
        self.log(f"Loaded {len(self.safe_macros)} safe macros from {len(self.all_macros)} total")
        
    def generate_macro_combination(self):
        """Îß§ÌÅ¨Î°ú Ï°∞Ìï© ÏÉùÏÑ±"""
        num_macros = random.randint(3, min(10, len(self.safe_macros)))
        selected_macros = random.sample(self.safe_macros, num_macros)
        
        macro_changes = {}
        for macro in selected_macros:
            # Î≤ÑÌçº ÌÅ¨Í∏∞ Í¥ÄÎ†® Îß§ÌÅ¨Î°úÎäî Îã§ÏñëÌïú Í∞í ÏãúÎèÑ
            if 'BUFLEN' in macro or 'SIZE' in macro:
                macro_changes[macro] = random.choice([64, 128, 256, 512, 1024, 2048])
            elif 'MAX' in macro:
                macro_changes[macro] = random.choice([1, 2, 4, 8, 16, 32])
            else:
                # ÏùºÎ∞ò Îß§ÌÅ¨Î°úÎäî 0/1
                macro_changes[macro] = random.choice([0, 1])
                
        return macro_changes
    
    def get_macro_hash(self, macro_changes):
        """Îß§ÌÅ¨Î°ú Ï°∞Ìï©Ïùò Ìï¥ÏãúÍ∞í ÏÉùÏÑ±"""
        sorted_macros = sorted(macro_changes.items())
        macro_str = json.dumps(sorted_macros)
        return hashlib.md5(macro_str.encode()).hexdigest()
        
    def apply_macro_changes(self, macro_changes, test_id):
        """ap_config.hÏóê Îß§ÌÅ¨Î°ú Î≥ÄÍ≤ΩÏÇ¨Ìï≠ Ï†ÅÏö©"""
        config_h = os.path.join(self.ardupilot_path, "build/AIRLink/ap_config.h")
        
        content = """#pragma once
/* ConfigFuzz generated configuration */

#ifndef _AP_CONFIG_H_
#define _AP_CONFIG_H_

/* Essential defines */
#define _GNU_SOURCE 1
#define WAF_BUILD 1
#define PYTHONDIR "/usr/lib/python3/dist-packages"
#define PYTHONARCHDIR "/usr/lib/python3/dist-packages"
#define __STDC_FORMAT_MACROS 1
#define AP_SIGNED_FIRMWARE 0
#define HAVE_CMATH_ISFINITE 1
#define HAVE_CMATH_ISINF 1
#define HAVE_CMATH_ISNAN 1
#define NEED_CMATH_ISFINITE_STD_NAMESPACE 1
#define NEED_CMATH_ISINF_STD_NAMESPACE 1
#define NEED_CMATH_ISNAN_STD_NAMESPACE 1

/* ConfigFuzz Test #{} */
""".format(test_id)
        
        for name, value in macro_changes.items():
            content += f"\n/* ConfigFuzz: {name} */\n"
            content += f"#ifdef {name}\n"
            content += f"#undef {name}\n"
            content += f"#endif\n"
            content += f"#define {name} {value}\n"
        
        content += "\n#endif /* _AP_CONFIG_H_ */\n"
        
        with open(config_h, 'w') as f:
            f.write(content)
            
        # ÏÑ§Ï†ï ÌååÏùºÎèÑ Ï†ÄÏû•
        config_copy = os.path.join(self.output_dir, "configs", f"test_{test_id:06d}_config.h")
        shutil.copy(config_h, config_copy)
        
    def analyze_stack_usage(self, test_id):
        """Ï†ÑÏ≤¥ .su ÌååÏùºÏóêÏÑú Ïä§ÌÉù ÏÇ¨Ïö©Îüâ Î∂ÑÏÑù (Í∞úÏÑ†Îêú Î≤ÑÏ†Ñ)"""
        high_stack_functions = []
        all_stack_functions = {}
        
        # Î™®Îì† .su ÌååÏùº Ï∞æÍ∏∞
        su_files = glob.glob(os.path.join(self.build_dir, "**/*.su"), recursive=True)
        
        for su_file in su_files:
            try:
                with open(su_file, 'r') as f:
                    for line in f:
                        parts = line.strip().split('\t')
                        if len(parts) >= 2:
                            func_name = parts[0]
                            try:
                                stack_size = int(parts[1])
                                
                                # Î™®Îì† Ìï®ÏàòÏùò Ïä§ÌÉù ÌÅ¨Í∏∞ Ï†ÄÏû•
                                all_stack_functions[func_name] = stack_size
                                
                                # ÏûÑÍ≥ÑÍ∞í Ïù¥ÏÉÅÏù∏ Ìï®ÏàòÎßå high_stackÏóê Ï∂îÍ∞Ä
                                if stack_size >= self.stack_threshold:
                                    high_stack_functions.append({
                                        "function": func_name,
                                        "stack_size": stack_size,
                                        "file": os.path.basename(su_file)
                                    })
                            except ValueError:
                                pass
            except:
                pass
                
        # Ïä§ÌÉù ÌÅ¨Í∏∞Î°ú Ï†ïÎ†¨
        high_stack_functions.sort(key=lambda x: x['stack_size'], reverse=True)
        
        # Ïä§ÌÉù Î≥ÄÌôî Î∂ÑÏÑù
        stack_changes = self.analyze_stack_changes(all_stack_functions, test_id)
        
        return high_stack_functions[:20], all_stack_functions, stack_changes  # ÏÉÅÏúÑ 20Í∞ú
        
    def analyze_stack_changes(self, current_stack_map, test_id):
        """Í∏∞Ï§Ä ÎπåÎìú ÎåÄÎπÑ Ïä§ÌÉù Î≥ÄÌôî Î∂ÑÏÑù"""
        stack_changes = {
            "new_functions": [],
            "removed_functions": [],
            "increased": [],
            "decreased": [],
            "unchanged": 0
        }
        
        # Ï≤´ Î≤àÏß∏ ÏÑ±Í≥µ ÎπåÎìúÎ•º Í∏∞Ï§ÄÏúºÎ°ú ÏÑ§Ï†ï (success_countÍ∞Ä 1Ïù¥ ÎêòÏóàÏùÑ Îïå)
        if not self.baseline_stack_map and self.success_count == 1:
            self.baseline_stack_map = current_stack_map.copy()
            self.log(f"[Test {test_id}] Set as baseline with {len(self.baseline_stack_map)} functions")
            return stack_changes
        
        # Í∏∞Ï§ÄÏù¥ ÏóÜÏúºÎ©¥ ÎπÑÍµêÌïòÏßÄ ÏïäÏùå
        if not self.baseline_stack_map:
            return stack_changes
        
        # Î≥ÄÌôî Î∂ÑÏÑù
        all_functions = set(self.baseline_stack_map.keys()) | set(current_stack_map.keys())
        
        for func in all_functions:
            baseline_size = self.baseline_stack_map.get(func, -1)
            current_size = current_stack_map.get(func, -1)
            
            if baseline_size == -1:
                # ÏÉàÎ°ú Ï∂îÍ∞ÄÎêú Ìï®Ïàò
                stack_changes["new_functions"].append({
                    "function": func,
                    "stack_size": current_size
                })
            elif current_size == -1:
                # Ï†úÍ±∞Îêú Ìï®Ïàò
                stack_changes["removed_functions"].append({
                    "function": func,
                    "stack_size": baseline_size
                })
            elif current_size > baseline_size:
                # Ïä§ÌÉù Ï¶ùÍ∞Ä
                diff = current_size - baseline_size
                if diff >= 16:  # 16Î∞îÏù¥Ìä∏ Ïù¥ÏÉÅ Ï∞®Ïù¥Îßå Í∏∞Î°ù
                    stack_changes["increased"].append({
                        "function": func,
                        "baseline": baseline_size,
                        "current": current_size,
                        "diff": diff
                    })
            elif current_size < baseline_size:
                # Ïä§ÌÉù Í∞êÏÜå
                diff = baseline_size - current_size
                if diff >= 16:  # 16Î∞îÏù¥Ìä∏ Ïù¥ÏÉÅ Ï∞®Ïù¥Îßå Í∏∞Î°ù
                    stack_changes["decreased"].append({
                        "function": func,
                        "baseline": baseline_size,
                        "current": current_size,
                        "diff": -diff
                    })
            else:
                stack_changes["unchanged"] += 1
        
        # Í∞ÄÏû• ÌÅ∞ Î≥ÄÌôîÎì§ÏùÑ Ï†ïÎ†¨
        stack_changes["increased"].sort(key=lambda x: x['diff'], reverse=True)
        stack_changes["decreased"].sort(key=lambda x: -x['diff'], reverse=True)
        
        return stack_changes
        
    def run_waf_build(self, test_id):
        """WafÎ°ú ÎπåÎìú Ïã§Ìñâ"""
        current_dir = os.getcwd()
        os.chdir(self.ardupilot_path)
        
        build_log = os.path.join(self.output_dir, "logs", f"test_{test_id:06d}_build.log")
        
        cmd = ["./waf", "build", "--target", "bin/arducopter", "-j4"]
        
        start_time = time.time()
        
        try:
            with open(build_log, 'w') as log_f:
                result = subprocess.run(
                    cmd,
                    stdout=log_f,
                    stderr=subprocess.STDOUT,
                    timeout=6000,
                    text=True
                )
                
            build_time = time.time() - start_time
            success = (result.returncode == 0)
            error_msg = None
            
            if not success:
                with open(build_log, 'r') as f:
                    lines = f.readlines()
                    for line in lines:
                        if 'error:' in line:
                            error_msg = line.strip()
                            break
                            
        except subprocess.TimeoutExpired:
            build_time = time.time() - start_time
            success = False
            error_msg = "Build timeout (120s)"
            
        except Exception as e:
            build_time = time.time() - start_time
            success = False
            error_msg = str(e)
            
        os.chdir(current_dir)
        
        return success, build_time, error_msg
        
    def run_single_test(self):
        """Îã®Ïùº ÌÖåÏä§Ìä∏ Ïã§Ìñâ"""
        self.test_count += 1
        test_id = self.test_count
        
        self.log(f"\n[Test {test_id}] Starting...")
        
        # Îß§ÌÅ¨Î°ú Ï°∞Ìï© ÏÉùÏÑ±
        macro_changes = self.generate_macro_combination()
        self.log(f"[Test {test_id}] Generated {len(macro_changes)} macro changes")
        
        # Ï∫êÏãú ÌôïÏù∏
        if self.use_cache:
            macro_hash = self.get_macro_hash(macro_changes)
            if macro_hash in self.macro_cache:
                self.cache_hits += 1
                cached_result = self.macro_cache[macro_hash]
                self.log(f"[Test {test_id}] Cache hit! Using previous result")
                
                success = cached_result['build_result']
                build_time = 0.1
                error_msg = cached_result.get('error')
                high_stack_functions = cached_result.get('high_stack_functions', [])
                stack_overflow_risk = cached_result.get('stack_overflow_risk', 0)
                stack_changes = cached_result.get('stack_changes', {})
                
            else:
                # Ïã§Ï†ú ÎπåÎìú ÏàòÌñâ
                self.apply_macro_changes(macro_changes, test_id)
                success, build_time, error_msg = self.run_waf_build(test_id)
                
                if success:
                    # Ïä§ÌÉù Î∂ÑÏÑù
                    high_stack_functions, all_stack_map, stack_changes = self.analyze_stack_usage(test_id)
                    stack_overflow_risk = len(high_stack_functions)
                    
                    # Ïä§ÌÉù Î∂ÑÏÑù Í≤∞Í≥º Ï†ÄÏû•
                    stack_file = os.path.join(self.output_dir, "stack_analysis", f"test_{test_id:06d}_stack.json")
                    with open(stack_file, 'w') as f:
                        json.dump({
                            "test_id": test_id,
                            "high_stack_count": len(high_stack_functions),
                            "high_stack_functions": high_stack_functions[:10],
                            "stack_changes": stack_changes,
                            "total_functions_analyzed": len(all_stack_map)
                        }, f, indent=2)
                else:
                    high_stack_functions = []
                    stack_overflow_risk = 0
                    stack_changes = {}
                
                # Ï∫êÏãúÏóê Ï†ÄÏû•
                self.macro_cache[macro_hash] = {
                    'build_result': success,
                    'build_time': build_time,
                    'error': error_msg,
                    'high_stack_functions': high_stack_functions[:10],
                    'stack_overflow_risk': stack_overflow_risk,
                    'stack_changes': stack_changes
                }
        else:
            # Ï∫êÏãú ÏóÜÏù¥ ÎπåÎìú
            self.apply_macro_changes(macro_changes, test_id)
            success, build_time, error_msg = self.run_waf_build(test_id)
            
            if success:
                high_stack_functions, all_stack_map, stack_changes = self.analyze_stack_usage(test_id)
                stack_overflow_risk = len(high_stack_functions)
            else:
                high_stack_functions = []
                stack_overflow_risk = 0
                stack_changes = {}
        
        # ÏÑ§Ï†ï Ï†ÄÏû•
        config_file = os.path.join(self.output_dir, "configs", f"test_{test_id:06d}_macros.json")
        with open(config_file, 'w') as f:
            json.dump({
                "test_id": test_id,
                "timestamp": datetime.now().isoformat(),
                "macro_changes": macro_changes
            }, f, indent=2)
        
        # Í≤∞Í≥º Î∂ÑÏÑù
        result = {
            "test_id": test_id,
            "timestamp": datetime.now().isoformat(),
            "macro_changes": macro_changes,
            "build_success": success,
            "build_time": build_time,
            "build_result": success,
        }
        
        if success:
            self.success_count += 1
            self.log(f"[Test {test_id}] ‚úÖ BUILD SUCCESS in {build_time:.1f}s")
            
            # Ïä§ÌÉù Î∂ÑÏÑù Í≤∞Í≥º Ï∂îÍ∞Ä
            result["high_stack_functions"] = high_stack_functions[:10]
            result["stack_overflow_risk"] = stack_overflow_risk
            result["stack_changes_summary"] = {
                "new_functions": len(stack_changes.get("new_functions", [])),
                "removed_functions": len(stack_changes.get("removed_functions", [])),
                "increased_stack": len(stack_changes.get("increased", [])),
                "decreased_stack": len(stack_changes.get("decreased", [])),
                "unchanged": stack_changes.get("unchanged", 0)
            }
            
            # Ï§ëÏöîÌïú Ïä§ÌÉù Î≥ÄÌôî Î°úÍπÖ
            if stack_changes.get("increased"):
                self.log(f"[Test {test_id}] ‚ö†Ô∏è  Stack increased in {len(stack_changes['increased'])} functions")
                for func in stack_changes['increased'][:3]:  # ÏÉÅÏúÑ 3Í∞úÎßå
                    self.log(f"  - {func['function'][:60]}: {func['baseline']} ‚Üí {func['current']} (+{func['diff']})")
                    
            if len(high_stack_functions) > 1:  # ppp_logit Ïô∏Ïóê Îã§Î•∏ Ìï®ÏàòÍ∞Ä ÏûàÏúºÎ©¥
                self.log(f"[Test {test_id}] üìä Found {len(high_stack_functions)} high stack functions")
                for i, func in enumerate(high_stack_functions[:3]):
                    self.log(f"  {i+1}. {func['stack_size']} bytes: {func['function'][:50]}...")
                    
        else:
            self.fail_count += 1
            self.log(f"[Test {test_id}] ‚ùå BUILD FAILED in {build_time:.1f}s")
            if error_msg:
                result["error"] = error_msg
                self.log(f"[Test {test_id}] Error: {error_msg[:100]}...")
                
        # Í≤∞Í≥º Ï†ÄÏû• (ConfigFuzz ÌòïÏãù)
        meta_file = os.path.join(self.output_dir, f"result_ardupilot_{test_id-1}.json_meta_results.json")
        with open(meta_file, 'w') as f:
            json.dump({
                "build_result": success,
                "start_time": time.time() - build_time,
                "end_time": time.time(),
                "total_time": build_time,
                "unique_stack_smash_count": stack_overflow_risk if success else 0
            }, f)
            
        config_results_file = os.path.join(self.output_dir, f"result_ardupilot_{test_id-1}.json_config_results.json")
        with open(config_results_file, 'w') as f:
            json.dump([result], f, indent=2)
            
        result_file = os.path.join(self.output_dir, "results", f"test_{test_id:06d}_result.json")
        with open(result_file, 'w') as f:
            json.dump(result, f, indent=2)
            
        return success
        
    def print_status(self):
        """ÌòÑÏû¨ ÏÉÅÌÉú Ï∂úÎ†•"""
        elapsed = time.time() - self.original_start_time
        remaining = self.timeout - (time.time() - self.start_time)
        
        self.log("\n" + "="*60)
        self.log(f"Status Update:")
        self.log(f"  Total elapsed: {self.format_duration(elapsed)}")
        self.log(f"  Session remaining: {self.format_duration(remaining)}")
        self.log(f"  Total tests: {self.test_count}")
        self.log(f"  Success: {self.success_count} ({self.success_count/self.test_count*100:.1f}%)")
        self.log(f"  Failed: {self.fail_count}")
        if self.use_cache:
            self.log(f"  Cache hits: {self.cache_hits} ({self.cache_hits/self.test_count*100:.1f}%)")
        self.log(f"  Rate: {self.test_count/(elapsed/3600):.1f} tests/hour")
        self.log(f"  Stack threshold: {self.stack_threshold} bytes")
        self.log("="*60)
        
    def save_cache(self):
        """Ï∫êÏãú Ï†ÄÏû•"""
        if self.use_cache:
            cache_file = os.path.join(self.output_dir, "macro_cache.json")
            with open(cache_file, 'w') as f:
                json.dump(self.macro_cache, f)
                
    def save_final_report(self):
        """ÏµúÏ¢Ö Î¶¨Ìè¨Ìä∏ Ï†ÄÏû•"""
        elapsed = time.time() - self.original_start_time
        
        # ÏÑ∏ÏÖò ÏÉÅÌÉú Ï†ÄÏû•
        self.save_session_state()
        
        # Ï∫êÏãú Ï†ÄÏû•
        self.save_cache()
        
        # Ïä§ÌÉù Î≥ÄÌôî ÌÜµÍ≥Ñ ÏàòÏßë
        stack_change_stats = {
            "total_changes": 0,
            "max_increase": 0,
            "max_decrease": 0,
            "functions_with_changes": set()
        }
        
        # stack_analysis ÎîîÎ†âÌÜ†Î¶¨Ïùò Î™®Îì† ÌååÏùº Î∂ÑÏÑù
        stack_files = glob.glob(os.path.join(self.output_dir, "stack_analysis", "*.json"))
        for stack_file in stack_files:
            try:
                with open(stack_file, 'r') as f:
                    data = json.load(f)
                    changes = data.get("stack_changes", {})
                    
                    for func in changes.get("increased", []):
                        stack_change_stats["total_changes"] += 1
                        stack_change_stats["max_increase"] = max(stack_change_stats["max_increase"], func["diff"])
                        stack_change_stats["functions_with_changes"].add(func["function"])
                        
                    for func in changes.get("decreased", []):
                        stack_change_stats["total_changes"] += 1
                        stack_change_stats["max_decrease"] = max(stack_change_stats["max_decrease"], -func["diff"])
                        stack_change_stats["functions_with_changes"].add(func["function"])
            except:
                pass
        
        report = {
            "session_info": {
                "start_time": datetime.fromtimestamp(self.start_time).isoformat(),
                "end_time": datetime.now().isoformat(),
                "duration_seconds": elapsed,
                "duration_formatted": self.format_duration(elapsed),
                "timeout_seconds": self.timeout,
                "timeout_formatted": self.format_duration(self.timeout)
            },
            "results": {
                "total_tests": self.test_count,
                "successful_builds": self.success_count,
                "failed_builds": self.fail_count,
                "success_rate": self.success_count / self.test_count * 100 if self.test_count > 0 else 0
            },
            "performance": {
                "tests_per_hour": self.test_count / (elapsed / 3600) if elapsed > 0 else 0,
                "average_build_time": elapsed / self.test_count if self.test_count > 0 else 0,
                "cache_hits": self.cache_hits if self.use_cache else 0,
                "cache_hit_rate": self.cache_hits / self.test_count * 100 if self.use_cache and self.test_count > 0 else 0
            },
            "stack_analysis": {
                "threshold": self.stack_threshold,
                "total_stack_changes": stack_change_stats["total_changes"],
                "max_stack_increase": stack_change_stats["max_increase"],
                "max_stack_decrease": stack_change_stats["max_decrease"],
                "functions_with_changes": len(stack_change_stats["functions_with_changes"])
            }
        }
        
        # JSON Î¶¨Ìè¨Ìä∏
        report_path = os.path.join(self.output_dir, "final_report.json")
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
            
        # ÌÖçÏä§Ìä∏ Î¶¨Ìè¨Ìä∏
        txt_report_path = os.path.join(self.output_dir, "final_report.txt")
        with open(txt_report_path, 'w') as f:
            f.write("ConfigFuzz + Waf Integration v2 Report\n")
            f.write("=====================================\n\n")
            f.write(f"Duration: {report['session_info']['duration_formatted']}\n")
            f.write(f"Total tests: {self.test_count}\n")
            f.write(f"Success rate: {report['results']['success_rate']:.1f}%\n")
            f.write(f"Average rate: {report['performance']['tests_per_hour']:.1f} tests/hour\n")
            f.write(f"\nSuccessful builds: {self.success_count}\n")
            f.write(f"Failed builds: {self.fail_count}\n")
            if self.use_cache:
                f.write(f"Cache hits: {self.cache_hits} ({report['performance']['cache_hit_rate']:.1f}%)\n")
            f.write(f"\nStack Analysis:\n")
            f.write(f"  Threshold: {self.stack_threshold} bytes\n")
            f.write(f"  Total stack changes: {stack_change_stats['total_changes']}\n")
            f.write(f"  Max increase: {stack_change_stats['max_increase']} bytes\n")
            f.write(f"  Max decrease: {stack_change_stats['max_decrease']} bytes\n")
            f.write(f"  Functions affected: {len(stack_change_stats['functions_with_changes'])}\n")
            
        self.log(f"\nFinal report saved to: {self.output_dir_name}/final_report.json")
        
    def run(self):
        """Î©îÏù∏ Ïã§Ìñâ Î£®ÌîÑ"""
        if not self.resumed_session:
            self.log("Starting ConfigFuzz + Waf fuzzing v2...")
        else:
            self.log("Resuming ConfigFuzz + Waf fuzzing v2...")
        
        # Ï¥àÍ∏∞ ÌôòÍ≤Ω ÏÑ§Ï†ï
        os.chdir(self.ardupilot_path)
        
        # Waf configure Ïã§Ìñâ (ÏÉà ÏÑ∏ÏÖòÏùº ÎïåÎßå)
        if not self.resumed_session:
            self.log("\nRunning waf configure...")
            result = subprocess.run(["./waf", "configure", "--board", "AIRLink"],
                                  capture_output=True, text=True)
            if result.returncode != 0:
                self.log("ERROR: Waf configure failed!")
                self.log(result.stderr[:500])
                return
            self.log("Waf configure successful")
        else:
            self.log("Skipping waf configure (resumed session)")
        
        # Î©îÏù∏ Î£®ÌîÑ
        try:
            while self.running and (time.time() - self.start_time) < self.timeout:
                self.run_single_test()
                
                # 10Í∞úÎßàÎã§ ÏÉÅÌÉú Ï∂úÎ†•
                if self.test_count % 10 == 0:
                    self.print_status()
                    
                # 20Í∞úÎßàÎã§ Ï∫êÏãúÏôÄ ÏÑ∏ÏÖò ÏÉÅÌÉú Ï†ÄÏû•
                if self.test_count % 20 == 0:
                    self.save_cache()
                    self.save_session_state()
                    
        except KeyboardInterrupt:
            self.log("\nInterrupted by user")
        except Exception as e:
            self.log(f"\nError occurred: {e}")
        finally:
            self.log(f"\nFuzzing completed! Results in: {self.output_dir_name}")
            self.log("="*60)
            self.log(f"Total: {self.test_count} tests")
            if self.test_count > 0:
                self.log(f"Success: {self.success_count} ({self.success_count/self.test_count*100:.1f}%)")
            else:
                self.log(f"Success: {self.success_count}")
            self.log(f"Failed: {self.fail_count}")
            if self.use_cache:
                self.log(f"Cache hits: {self.cache_hits}")
            self.save_final_report()
            self.main_log.close()


def main():
    parser = argparse.ArgumentParser(
        description="ConfigFuzz + Waf Integration v2 with Resume Support",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Start new session
  python3 configfuzz_waf_runner_v2.py --hours 6
  
  # Resume previous session
  python3 configfuzz_waf_runner_v2.py --resume output_waf_v2_20250812_135802 --hours 3
  
  # Resume with just directory name
  python3 configfuzz_waf_runner_v2.py --resume output_waf_v2_20250812_135802
  
  # Other options
  python3 configfuzz_waf_runner_v2.py --minutes 30      # Run for 30 minutes
  python3 configfuzz_waf_runner_v2.py --no-cache        # Disable cache
  python3 configfuzz_waf_runner_v2.py --threshold 256   # Use 256 byte threshold
        """
    )
    
    # Resume argument
    parser.add_argument(
        "--resume",
        type=str,
        help="Resume from previous session directory"
    )
    
    # Time arguments (mutually exclusive)
    time_group = parser.add_mutually_exclusive_group()
    time_group.add_argument(
        "--hours",
        type=float,
        help="Duration in hours"
    )
    time_group.add_argument(
        "--minutes", "-m",
        type=float,
        help="Duration in minutes"
    )
    time_group.add_argument(
        "--seconds", "-s",
        type=int,
        help="Duration in seconds"
    )
    
    # Other arguments
    parser.add_argument(
        "--no-cache",
        action="store_true",
        help="Disable macro combination caching"
    )
    
    parser.add_argument(
        "--threshold", "-t",
        type=int,
        default=512,
        help="Stack size threshold in bytes (default: 512)"
    )
    
    args = parser.parse_args()
    
    # ÏãúÍ∞Ñ Í≥ÑÏÇ∞
    if args.hours:
        timeout = int(args.hours * 3600)
    elif args.minutes:
        timeout = int(args.minutes * 60)
    elif args.seconds:
        timeout = args.seconds
    else:
        # Í∏∞Î≥∏Í∞í: 1ÏãúÍ∞Ñ
        timeout = 3600
        
    # Ï∫êÏãú ÏÑ§Ï†ï
    use_cache = not args.no_cache
    
    # Ïã§Ìñâ
    fuzzer = ConfigFuzzWafRunnerV2(
        timeout_seconds=timeout, 
        use_cache=use_cache,
        stack_threshold=args.threshold,
        resume_from=args.resume
    )
    fuzzer.run()


if __name__ == "__main__":
    main()
